{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "direct-interest",
   "metadata": {},
   "source": [
    "# NESM Python Part 4 - Advanced Topics\n",
    "\n",
    "- Deep learning with Tensorflow\n",
    "- Our image analysis pipeline at a glance\n",
    "- Dask for out of memory computing\n",
    "- Classical machine learning with `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from mpl_interactions import hyperslicer\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-review",
   "metadata": {},
   "source": [
    "## Dask for out of memory computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = 8e9 #8GB \n",
    "pixels = 1024*1024\n",
    "bytes_per_pix = 2 #16 bit unsigned ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory/(pixels*bytes_per_pix) #images you can have in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-courtesy",
   "metadata": {},
   "source": [
    "That seems like a lot but that corresponds to less than\n",
    "\n",
    "(20 Time points) x (10 Positions) x (4 Channels) x (5 z-slices) = 4000 Images\n",
    "\n",
    "\n",
    "**Enter Dask Array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "20*10*4*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impossible_arr = np.random.random((10000,1024,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr = da.random.random((10000, 1024, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "(darr - darr.min())/(darr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch a client from dask-labextension\n",
    "# scale to 3 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, the oldschool way\n",
    "# from dask.distributed import Client\n",
    "# client = Client()\n",
    "# client.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = darr.mean(0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-anatomy",
   "metadata": {},
   "source": [
    "### Loading data with dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have had a hard time with dask's imread in the past, here it works great, your mileage may vary\n",
    "# Also it appears to need the absolute path to the data\n",
    "cho_arr = da.image.imread(os.getcwd()+'/Fluo-N3DH-CHO/01/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "cho_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-train",
   "metadata": {},
   "source": [
    "### Dask + Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {'T':9.5*np.arange(cho_arr.shape[0]), \n",
    "          'Z':1.0*np.arange(cho_arr.shape[1]),\n",
    "          'Y':0.202*np.arange(cho_arr.shape[2]),\n",
    "          'X':0.202*np.arange(cho_arr.shape[3])}\n",
    "x_data = xr.DataArray(cho_arr, dims=coords.keys(), coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "controls = hyperslicer(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-sherman",
   "metadata": {},
   "source": [
    "**Note about Dask**\n",
    "\n",
    "- One of my favorite things about dask is that I can develop on my laptop and run with 4 cores but then move to Harvard's computing cluster and run with much more computing power. Dask scales seamlessly between these two settings.\n",
    "\n",
    "\n",
    "- Dask maintains several different APIs. I'd recommend [this page from their documentation](https://docs.dask.org/en/latest/user-interfaces.html) to see what would work for you. In brief there are high level interfaces:\n",
    "  - Array - for data that is a high dimensional rectangle - Will take you far for large imaging datasets and is likely the easiest to use.\n",
    "  - Dataframe - for tabular data. Array:Numpy :: Dataframe:Pandas\n",
    "  - Bag - more like a database format that implements Map-Reduce type operations.\n",
    "  - Dask-ML - scikit-learn (more on this below) like interface for scaling machine learning tasks.\n",
    " \n",
    " \n",
    "- There are also lower level interfaces for custom computation\n",
    "  - Delayed - For custom python computation that does not necessarily fit the array paradigm. *Importantly* you set up all your computation and tell dask when evaluate it.\n",
    "  - Futures - *Dynamic* custom computation. Things start running in real time and dask decides when things run by evaluating which computations depend on other computations. This is likely the most powerful and most confusing interface. Example: Return a list of some length and do operations on all elements.\n",
    "  \n",
    "- Keep an eye on [`dask_image`](http://image.dask.org/en/latest/) which is gradually wrapping scipy.ndimage functionality for dask arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-waterproof",
   "metadata": {},
   "source": [
    "## PCA on Hyperspectral SRS imaging data\n",
    "\n",
    "**What is PCA?**\n",
    "\n",
    "Principal component analysis (PCA) finds the basis vectors which explain most of the variance in a dataset. Below is a picture from the [wikipedia page](https://en.wikipedia.org/wiki/Principal_component_analysis) which shows the principal components of some correlated 2D data. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/2560px-GaussianScatterPCA.svg.png\" width=\"500\"/>\n",
    "\n",
    "**What is SRS?**\n",
    "\n",
    "[Stimulated Raman Scattering](https://en.wikipedia.org/wiki/Stimulated_Raman_spectroscopy) (SRS) is an optical imaging technique that probes the vibrational energy levels of different molecules. I'm using it to study cellular metabolism and composition but its generally good for chemical mapping of materials with different vibrational energy levels.\n",
    "\n",
    "The toy dataset below is a spectral scan of two different species of beads. We will use PCA to \"discover\" how many different species are in the sample and what their spectra look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset directly from github (33MB)\n",
    "# Feel free to just watch if you dont want to download\n",
    "response = requests.get(\n",
    "    \"https://github.com/jrussell25/data-sharing/raw/master/srs_beads.npy\"\n",
    ")\n",
    "response.raise_for_status()\n",
    "beads = np.load(io.BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "beads.nbytes/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the coordinates for the xarray as a dict of name:array pairs\n",
    "# Wns = Wns is relevant spectroscopic unit in cm^-1 as above\n",
    "# X,Y = actual dimensions of the images in microns from microscope metadata\n",
    "coords = {'wavenums':np.linspace(2798.65, 3064.95, beads.shape[0]),\n",
    "          'X':np.linspace(0, 386.44, 512),\n",
    "          'Y':np.linspace(0, 386.44,512)}\n",
    "\n",
    "x_beads = xr.DataArray(beads, dims=coords.keys(), coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ctrls = hyperslicer(x_beads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-import",
   "metadata": {},
   "source": [
    "### How to do PCA in python?\n",
    "\n",
    "Of course we google it first.\n",
    "\n",
    "But the answer is [scikit-learn](https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do some annoying reshapeing because sklearn expects (N_data, N_features)\n",
    "pcs = pca.fit_transform(beads.reshape(beads.shape[0], -1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of 126 spectral points, we have 10 features corresponding to the first 10 PCs\n",
    "pcs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_beads['wavenums'],pca.components_[:5].T + np.arange(5)[None,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-butter",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a fun visualization\n",
    "rgb = pcs[...,:3].reshape(512, 512,3)\n",
    "rgb = (rgb-rgb.min(0).min(0))\n",
    "rgb = rgb/rgb.max(0).max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(rgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
