{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afraid-trick",
   "metadata": {},
   "source": [
    "# NESM Python Part 4 - Advanced Topics\n",
    "\n",
    "- Deep learning with Tensorflow\n",
    "- Our image analysis pipeline at a glance\n",
    "- Dask for out of memory computing\n",
    "- Classical machine learning with `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from mpl_interactions import hyperslicer\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-greensboro",
   "metadata": {},
   "source": [
    "## Dask for out of memory computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = 8e9 #8GB \n",
    "pixels = 1024*1024\n",
    "bytes_per_pix = 2 #16 bit unsigned ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory/(pixels*bytes_per_pix) #images you can have in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-genetics",
   "metadata": {},
   "source": [
    "That seems like a lot but that corresponds to less than\n",
    "\n",
    "(20 Time points) x (10 Positions) x (4 Channels) x (5 z-slices) = 4000 Images\n",
    "\n",
    "\n",
    "**Enter Dask Array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "20*10*4*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "invisible-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da # To use dask you can call da.xxx when you would have called np.xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impossible_arr = np.random.random((10000,1024,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr = da.random.random((10000, 1024, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-center",
   "metadata": {},
   "source": [
    "What is this thing? Dask arrays function numpy arrays but they are *lazy*. This means that rather than storing all the information of the array in memory, dask keeps track of a *graph* that describes all the computation necessary to generate the array.\n",
    "\n",
    "We can continue to do operations on this array and they will all be lazy. Again this means dask just adds computations to the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "(darr - darr.min())/(darr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch a client from dask-labextension\n",
    "# scale to 3 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, the oldschool way\n",
    "# from dask.distributed import Client\n",
    "# client = Client()\n",
    "# client.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-player",
   "metadata": {},
   "source": [
    "When we actually want to evaluate an array, we call `.compute()` on it. You should watch the dask dashboard (by following the link above). Also beware that you're computer is about to start running aggressively so if you have low battery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = darr.mean(0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-study",
   "metadata": {},
   "source": [
    "### Loading data with dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have had a hard time with dask's imread in the past, here it works great, your mileage may vary\n",
    "# Also it appears to need the absolute path to the data\n",
    "cho_arr = da.image.imread(os.getcwd()+'/Fluo-N3DH-CHO/01/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "cho_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-stylus",
   "metadata": {},
   "source": [
    "### Dask + Xarray\n",
    "\n",
    "Xarray supports any "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {'T':9.5*np.arange(cho_arr.shape[0]), \n",
    "          'Z':1.0*np.arange(cho_arr.shape[1]),\n",
    "          'Y':0.202*np.arange(cho_arr.shape[2]),\n",
    "          'X':0.202*np.arange(cho_arr.shape[3])}\n",
    "x_data = xr.DataArray(cho_arr, dims=coords.keys(), coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "controls = hyperslicer(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-scanning",
   "metadata": {},
   "source": [
    "**Note about Dask**\n",
    "\n",
    "- One of my favorite things about dask is that I can develop on my laptop and run with 4 cores but then move to Harvard's computing cluster and run with much more computing power. Dask scales seamlessly between these two settings.\n",
    "\n",
    "\n",
    "- Dask maintains several different APIs. I'd recommend [this page from their documentation](https://docs.dask.org/en/latest/user-interfaces.html) to see what would work for you. In brief there are high level interfaces:\n",
    "  - Array - for data that is a high dimensional rectangle - Will take you far for large imaging datasets and is likely the easiest to use.\n",
    "  - Dataframe - for tabular data. Array:Numpy :: Dataframe:Pandas\n",
    "  - Bag - more like a database format that implements Map-Reduce type operations.\n",
    "  - Dask-ML - scikit-learn (more on this below) like interface for scaling machine learning tasks.\n",
    " \n",
    " \n",
    "- There are also lower level interfaces for custom computation\n",
    "  - Delayed - For custom python computation that does not necessarily fit the array paradigm. *Importantly* you set up all your computation and tell dask when evaluate it.\n",
    "  - Futures - *Dynamic* custom computation. Things start running in real time and dask decides when things run by evaluating which computations depend on other computations. This is likely the most powerful and most confusing interface. Example: Return a list of some length and do operations on all elements.\n",
    "  \n",
    "- Keep an eye on [`dask_image`](http://image.dask.org/en/latest/) which is gradually wrapping scipy.ndimage functionality for dask arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-inside",
   "metadata": {},
   "source": [
    "## PCA on Hyperspectral SRS imaging data\n",
    "\n",
    "**What is PCA?**\n",
    "\n",
    "Principal component analysis (PCA) finds the basis vectors which explain most of the variance in a dataset. Below is a picture from the [wikipedia page](https://en.wikipedia.org/wiki/Principal_component_analysis) which shows the principal components of some correlated 2D data. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/2560px-GaussianScatterPCA.svg.png\" width=\"500\"/>\n",
    "\n",
    "**What is SRS?**\n",
    "\n",
    "[Stimulated Raman Scattering](https://en.wikipedia.org/wiki/Stimulated_Raman_spectroscopy) (SRS) is an optical imaging technique that probes the vibrational energy levels of different molecules. I'm using it to study cellular metabolism and composition but its generally good for chemical mapping of materials with different vibrational energy levels.\n",
    "\n",
    "The toy dataset below is a spectral scan of two different species of beads. We will use PCA to \"discover\" how many different species are in the sample and what their spectra look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset directly from github (33MB)\n",
    "# Feel free to just watch if you dont want to download\n",
    "response = requests.get(\n",
    "    \"https://github.com/jrussell25/data-sharing/raw/master/srs_beads.npy\"\n",
    ")\n",
    "response.raise_for_status()\n",
    "beads = np.load(io.BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the coordinates for the xarray as a dict of name:array pairs\n",
    "# Wavenums is the relevant spectroscopic unit in cm^-1\n",
    "# X,Y = actual dimensions of the images in microns from microscope metadata\n",
    "coords = {'wavenums':np.linspace(2798.65, 3064.95, beads.shape[0]),\n",
    "          'X':np.linspace(0, 386.44, 512),\n",
    "          'Y':np.linspace(0, 386.44,512)}\n",
    "\n",
    "x_beads = xr.DataArray(beads, dims=coords.keys(), coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ctrls = hyperslicer(x_beads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-colon",
   "metadata": {},
   "source": [
    "### How to do PCA in python?\n",
    "\n",
    "Of course we google it first.\n",
    "\n",
    "But the answer is [scikit-learn](https://scikit-learn.org/stable/). The `scikit-learn` (or `sklearn` for short) user manual is exceptionally good and always contains an example. Googling `sklearn some new method` is always the first thing I do when I want to try out a machine learning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do some annoying reshapeing because sklearn expects (N_data, N_features)\n",
    "# N_data = N_pixels = 512*512 and N_features = 126 spectral points\n",
    "pcs = pca.fit_transform(beads.reshape(beads.shape[0], -1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of 126 spectral points, we have 10 features corresponding to the first 10 PCs\n",
    "pcs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the principal components with \n",
    "plt.figure()\n",
    "plt.plot(x_beads['wavenums'],pca.components_[:5].T + np.arange(5)[None,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a fun visualization\n",
    "# Use the weights corresponding to the first 3 PCs as RGB values\n",
    "rgb = pcs[...,:3].reshape(512, 512,3)\n",
    "# for matplitlib, we need to normalize each color channel to be between 0 and 1\n",
    "rgb = (rgb-rgb.min(0).min(0))\n",
    "rgb = rgb/rgb.max(0).max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(rgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
